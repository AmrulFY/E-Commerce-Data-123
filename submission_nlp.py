# -*- coding: utf-8 -*-
"""Submission NLP V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jami8ikXhsR9iDiQ-wOYGV1upK0h1-QT

Nama : Amrul Fadhil Yofan

Dataset : https://www.kaggle.com/datasets/bhavikjikadara/fake-news-detection

Source : Kaggle
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout
from tensorflow.keras.models import Sequential
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize,sent_tokenize
from bs4 import BeautifulSoup
import re,string,unicodedata
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.model_selection import train_test_split
from string import punctuation
from nltk import pos_tag
from nltk.corpus import wordnet
import keras
from keras.preprocessing import text, sequence
import nltk
from collections import Counter

"""## Import Dataset dari Kaggle"""

!pip install -q kaggle

#Upload kaggle.json secara manual
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

# Download dataset
!kaggle datasets download -d bhavikjikadara/fake-news-detection

# Unzip dan lihat list dataset
!mkdir news
!unzip fake-news-detection.zip -d news
!ls news

#Import data
df_fake = pd.read_csv('news/fake.csv')
df_true = pd.read_csv('news/true.csv')
df_fake.head()

df_true.head()

"""## Analisis Data dan Cleaning Data"""

#Cek ukuran data
print(df_fake.shape)
print(df_true.shape)

#Cek data kosong
df_fake.isnull().sum()

df_true.isnull().sum()

"""Tujuan submission ini adalah untuk memprediksi suatu teks berita apakah asli atau palsu. Dalam hal ini, dua dataset yang berbeda akan digabung dan diberi label terlebih dahulu.

**Label**

0 untuk berita palsu

1 untuk berita asli
"""

df_fake.info()

df_true.info()

df_fake['label'] = 0
df_fake = df_fake.drop(columns=['date', 'subject'])
df_fake

df_true['label'] = 1
df_true = df_true.drop(columns=['date', 'subject'])
df_true

#Menggabungkan dua dataset
df = pd.concat([df_fake, df_true])
df

df.info()

df['label'].value_counts()

df['all_text'] = df['title'] + " " + df['text']
df

nltk.download('stopwords')

#Menghapus stopwords dan teks yang tidak terlalu penting

stwd = set(stopwords.words('english'))
punctuation = list(string.punctuation)
stwd.update(punctuation)

def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

#Menghapus square brackets
def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

#Menghapus URL's
def remove_url(text):
    return re.sub(r'http\S+', '', text)

#Menghapus stopwords dari text
def remove_stopwords(text):
    final_text = []
    for i in text.split():
        if i.strip().lower() not in stwd:
            final_text.append(i.strip())
    return " ".join(final_text)

#Menghapus keseluruhan (noisy text)
def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    text = remove_url(text)
    text = remove_stopwords(text)
    return text

df['all_text'] = df['all_text'].apply(denoise_text)

#Mendeteksi jumlah kata
def get_corpus(text):
    words = []
    for i in text:
        for j in i.split():
            words.append(j.strip())
    return words
corpus = get_corpus(df['all_text'])
corpus[:10]

#Jumlah kata yang sering ditemukan
counter = Counter(corpus)
most_common = counter.most_common(10)
most_common = dict(most_common)
most_common

"""## Split Data"""

length = df['all_text'].str.len().max()
length

text = df['all_text'].values
label = df['label'].values

#Split dataset 20%
x_train,x_test,y_train,y_test = train_test_split(text, label,test_size = 0.2, random_state=42)

print(x_train)

print(x_test)

print(y_train)
print(y_test)

"""##Tokenization dan Padding Sequence"""

max_len = 40
trunc_type = "post"
oov_tok = "<OOV>"

tokenizer = Tokenizer(num_words=length, oov_token=oov_tok)
tokenizer.fit_on_texts(x_train)
word_index = tokenizer.word_index
sequences_train = tokenizer.texts_to_sequences(x_train)
sequences_test = tokenizer.texts_to_sequences(x_test)
pad_train = pad_sequences(sequences_train, maxlen=max_len, truncating=trunc_type)
pad_test = pad_sequences(sequences_test, maxlen=max_len, truncating=trunc_type)

print(pad_train.shape)
print(pad_test.shape)

pad_train

pad_test

"""## Modelling"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=length, output_dim=64, input_length=max_len),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
Adam(learning_rate=0.00146, name='Adam')
model.compile(optimizer='Adam', metrics=['accuracy'], loss='binary_crossentropy')
model.summary()

#Callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
      self.model.stop_training = True
      print("\n Akurasi > 90% sehingga iterasi dihentikan")
callbacks = myCallback()

num_epochs = 50
history = model.fit(pad_train, y_train, epochs=num_epochs, validation_data=(pad_test, y_test), verbose=2, batch_size=128, callbacks=[callbacks])

"""Karena hanya membutuhkan 1 Epoch, maka grafik loss & accuracy di bawah bisa diabaikan."""

# Grafik Loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['train', 'test'], loc = 'upper right')
plt.show()

# Grafik Accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()

